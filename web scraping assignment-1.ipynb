{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da48d16",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "   assignment-1/answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab66f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#1)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract header tags from HTML content\n",
    "def extract_headers(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    return [header.text.strip() for header in headers]\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # Wikipedia URL\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "    # Fetch HTML content\n",
    "    html_content = get_html_content(wikipedia_url)\n",
    "\n",
    "    # Extract header tags\n",
    "    headers = extract_headers(html_content)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'Headers': headers})\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29200d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m html_content \u001b[38;5;241m=\u001b[39m get_html_content(presidents_url)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Extract information about former presidents\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m former_presidents_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_former_presidents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[0;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(former_presidents_data)\n",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m, in \u001b[0;36mextract_former_presidents\u001b[1;34m(html_content)\u001b[0m\n\u001b[0;32m     16\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable-responsive\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Extracting rows from the table\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Extracting information about each former president\u001b[39;00m\n\u001b[0;32m     22\u001b[0m presidents_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#2)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract information about former presidents\n",
    "def extract_former_presidents(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting the table containing the information about former presidents\n",
    "    table = soup.find('table', {'class': 'table-responsive'})\n",
    "    \n",
    "    # Extracting rows from the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    # Extracting information about each former president\n",
    "    presidents_data = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        term_of_office = columns[1].text.strip()\n",
    "        presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "    return presidents_data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URL of the page containing information about former presidents\n",
    "    presidents_url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "    # Fetch HTML content\n",
    "    html_content = get_html_content(presidents_url)\n",
    "\n",
    "    # Extract information about former presidents\n",
    "    former_presidents_data = extract_former_presidents(html_content)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(former_presidents_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46593ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_bowlers)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Fetch and create DataFrame for the top 10 ODI teams\u001b[39;00m\n\u001b[0;32m     44\u001b[0m teams_html_content \u001b[38;5;241m=\u001b[39m get_html_content(teams_url)\n\u001b[1;32m---> 45\u001b[0m top_10_teams_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_top_10_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteams_html_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m df_teams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(top_10_teams_data)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Fetch and create DataFrame for the top 10 ODI batsmen\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mextract_top_10_data\u001b[1;34m(html_content)\u001b[0m\n\u001b[0;32m     16\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Extracting rows from the table\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Extracting information about each team/batsman/bowler\u001b[39;00m\n\u001b[0;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#3)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract data for the top 10 teams, batsmen, or bowlers\n",
    "def extract_top_10_data(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting the relevant table\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extracting rows from the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    # Extracting information about each team/batsman/bowler\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        matches = columns[3].text.strip()\n",
    "        points = columns[4].text.strip()\n",
    "        rating = columns[5].text.strip()\n",
    "\n",
    "        data.append({'Rank': rank, 'Name': name, 'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URLs for the top 10 ODI teams, batsmen, and bowlers in men's cricket\n",
    "    teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 ODI teams\n",
    "    teams_html_content = get_html_content(teams_url)\n",
    "    top_10_teams_data = extract_top_10_data(teams_html_content)\n",
    "    df_teams = pd.DataFrame(top_10_teams_data)\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 ODI batsmen\n",
    "    batsmen_html_content = get_html_content(batsmen_url)\n",
    "    top_10_batsmen_data = extract_top_10_data(batsmen_html_content)\n",
    "    df_batsmen = pd.DataFrame(top_10_batsmen_data)\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 ODI bowlers\n",
    "    bowlers_html_content = get_html_content(bowlers_url)\n",
    "    top_10_bowlers_data = extract_top_10_data(bowlers_html_content)\n",
    "    df_bowlers = pd.DataFrame(top_10_bowlers_data)\n",
    "\n",
    "    # Display DataFrames\n",
    "    print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "    print(df_teams)\n",
    "\n",
    "    print(\"\\nTop 10 ODI Batsmen in Men's Cricket:\")\n",
    "    print(df_batsmen)\n",
    "\n",
    "    print(\"\\nTop 10 ODI Bowlers in Men's Cricket:\")\n",
    "    print(df_bowlers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92eb3a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_all_rounders)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Fetch and create DataFrame for the top 10 ODI teams\u001b[39;00m\n\u001b[0;32m     44\u001b[0m teams_html_content \u001b[38;5;241m=\u001b[39m get_html_content(teams_url)\n\u001b[1;32m---> 45\u001b[0m top_10_teams_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_top_10_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteams_html_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m df_teams \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(top_10_teams_data)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Fetch and create DataFrame for the top 10 women's ODI batting players\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m, in \u001b[0;36mextract_top_10_data\u001b[1;34m(html_content)\u001b[0m\n\u001b[0;32m     16\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Extracting rows from the table\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Extracting information about each team/player/all-rounder\u001b[39;00m\n\u001b[0;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#4)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract data for the top 10 teams, women's batting players, or women's all-rounders\n",
    "def extract_top_10_data(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting the relevant table\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extracting rows from the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    # Extracting information about each team/player/all-rounder\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        matches = columns[3].text.strip()\n",
    "        points = columns[4].text.strip()\n",
    "        rating = columns[5].text.strip()\n",
    "\n",
    "        data.append({'Rank': rank, 'Name': name, 'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URLs for the top 10 ODI teams, women's ODI batting players, and women's ODI all-rounders\n",
    "    teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    batting_players_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    all_rounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 ODI teams\n",
    "    teams_html_content = get_html_content(teams_url)\n",
    "    top_10_teams_data = extract_top_10_data(teams_html_content)\n",
    "    df_teams = pd.DataFrame(top_10_teams_data)\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 women's ODI batting players\n",
    "    batting_players_html_content = get_html_content(batting_players_url)\n",
    "    top_10_batting_players_data = extract_top_10_data(batting_players_html_content)\n",
    "    df_batting_players = pd.DataFrame(top_10_batting_players_data)\n",
    "\n",
    "    # Fetch and create DataFrame for the top 10 women's ODI all-rounders\n",
    "    all_rounders_html_content = get_html_content(all_rounders_url)\n",
    "    top_10_all_rounders_data = extract_top_10_data(all_rounders_html_content)\n",
    "    df_all_rounders = pd.DataFrame(top_10_all_rounders_data)\n",
    "\n",
    "    # Display DataFrames\n",
    "    print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "    print(df_teams)\n",
    "\n",
    "    print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "    print(df_batting_players)\n",
    "\n",
    "    print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "    print(df_all_rounders)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa54522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#5)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract news details\n",
    "def extract_news_details(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting news articles\n",
    "    articles = soup.find_all('div', {'class': 'Card-title'})\n",
    "\n",
    "    # Extracting information about each article\n",
    "    data = []\n",
    "    for article in articles:\n",
    "        headline = article.find('h3').text.strip()\n",
    "        time = article.find('time').text.strip()\n",
    "        news_link = article.find('a')['href']\n",
    "\n",
    "        data.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URL for the news page\n",
    "    news_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "    # Fetch HTML content\n",
    "    html_content = get_html_content(news_url)\n",
    "\n",
    "    # Extract news details\n",
    "    news_data = extract_news_details(html_content)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(news_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff23da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#6)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract details of most downloaded articles\n",
    "def extract_most_downloaded_articles(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting articles\n",
    "    articles = soup.find_all('div', {'class': 'text-xs'})\n",
    "\n",
    "    # Extracting information about each article\n",
    "    data = []\n",
    "    for article in articles:\n",
    "        title = article.find('a', {'class': 'anchor-text'}).text.strip()\n",
    "        authors = article.find('span', {'class': 'js-article-author'}).text.strip()\n",
    "        published_date = article.find('span', {'class': 'text-xs'}).text.strip()\n",
    "        paper_url = article.find('a', {'class': 'anchor-text'})['href']\n",
    "\n",
    "        data.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URL for the most downloaded articles page\n",
    "    articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "    # Fetch HTML content\n",
    "    html_content = get_html_content(articles_url)\n",
    "\n",
    "    # Extract most downloaded articles details\n",
    "    articles_data = extract_most_downloaded_articles(html_content)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(articles_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bd144ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#7)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def get_html_content(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "# Function to extract restaurant details\n",
    "def extract_restaurant_details(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extracting restaurants\n",
    "    restaurants = soup.find_all('div', {'class': 'restnt-info-wrap'})\n",
    "\n",
    "    # Extracting information about each restaurant\n",
    "    data = []\n",
    "    for restaurant in restaurants:\n",
    "        name = restaurant.find('div', {'class': 'restnt-info cursor-pointer'}).find('h4').text.strip()\n",
    "        cuisine = restaurant.find('span', {'class': 'double-line-ellipsis'}).text.strip()\n",
    "        location = restaurant.find('div', {'class': 'restnt-info cursor-pointer'}).find('p', {'class': 'double-line-ellipsis'}).text.strip()\n",
    "        ratings = restaurant.find('div', {'class': 'rating-widget'}).text.strip()\n",
    "        image_url = restaurant.find('img')['src']\n",
    "\n",
    "        data.append({'Restaurant Name': name, 'Cuisine': cuisine, 'Location': location, 'Ratings': ratings, 'Image URL': image_url})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main function to fetch data and create DataFrame\n",
    "def main():\n",
    "    # URL for the restaurants page on dineout.co.in\n",
    "    restaurants_url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "    # Fetch HTML content\n",
    "    html_content = get_html_content(restaurants_url)\n",
    "\n",
    "    # Extract restaurant details\n",
    "    restaurants_data = extract_restaurant_details(html_content)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(restaurants_data)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d3bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
